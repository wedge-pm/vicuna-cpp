{
  "name": "vicuna-cpp",
  "weights": [
    "https://huggingface.co/chharlesonfire/ggml-vicuna-7b-4bit/resolve/main/ggml-vicuna-7b-q4_0.bin"
  ],
  "flake": "github:ggerganov/llama.cpp",
  "setup": "npm --prefix ./server install",
  "options": {
    "model": {},
    "agent": {}
  },
  "serve": "node server/server.js"
}
